model = "Qwen/Qwen3-30B-A3B-Instruct-2507"
max_steps = 100
batch_size = 24
rollouts_per_example = 8

# Multi-agent training requires at least 1 LoRA adapter (attacker uses lora_id=0)
# Defender is non-trainable and uses the base model
max_runs = 1

[sampling]
max_tokens = 16384

[[env]]
id = "wambosec/multi-agent-injection"
args = { defense_levels = ["hard"], max_turns = 6, seed = 187 }
