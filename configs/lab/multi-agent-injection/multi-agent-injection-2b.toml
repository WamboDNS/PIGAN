max_steps = 100
seq_len = 16384
output_dir = "outputs_v1"

# Disable checkpointing
# ckpt is omitted entirely

[model]
name = "Qwen/Qwen3-1.7B"

[wandb]
project = "multi-agent-injection"
name = "multi-agent-injection-2b"

[trainer.model]
impl = "auto"

[trainer.model.ac]
freq = 1

[trainer.model.lora]
rank = 32
alpha = 64

[trainer.optim]
lr = 1e-5

# Multi-agent training requires max_concurrent_runs >= max(lora_id) + 1
# Attacker uses lora_id=0, defender uses base model (lora_id=None)
[trainer]
max_concurrent_runs = 1

[orchestrator]
batch_size = 128
rollouts_per_example = 16

[orchestrator.sampling]
max_tokens = 16384

[[orchestrator.env]]
id = "wambosec/multi-agent-injection"
name = "multi-agent-injection"
args = { defense_levels = ["medium"], max_turns = 6, seed = 187 }

[trainer.log]
env_worker_logs = true

[orchestrator.log]
env_worker_logs = true
level = "debug"
vf_level = "debug"

[inference]
