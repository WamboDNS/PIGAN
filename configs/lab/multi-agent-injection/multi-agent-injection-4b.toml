max_steps = 100
seq_len = 16384

[ckpt]

[model]
name = "Qwen/Qwen3-4B-Instruct-2507"

[wandb]
project = "multi-agent-injection"
name = "multi-agent-injection-4b"

[trainer.model]
impl = "auto"

[trainer.model.ac]
freq = 1

[trainer.model.lora]
rank = 32
alpha = 64

[trainer.optim]
lr = 1e-5

# Multi-agent training requires max_concurrent_runs >= max(lora_id) + 1
# Attacker uses lora_id=0, defender uses base model (lora_id=None)
[trainer]
max_concurrent_runs = 1

[orchestrator]
batch_size = 32
rollouts_per_example = 8

[orchestrator.sampling]
max_tokens = 16384

[[orchestrator.env]]
id = "multi-agent-injection"
name = "multi-agent-injection"
args = { defense_levels = ["hard"], max_turns = 6, seed = 187 }

[trainer.log]
env_worker_logs = true

[orchestrator.log]
env_worker_logs = true

[inference]
